{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Editing graph state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install --quiet -U langgraph langchain_openai langgraph_sdk langgraph-prebuilt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Editing state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply a and b.\n",
    "\n",
    "    Args:\n",
    "        a: first int\n",
    "        b: second int\n",
    "    \"\"\"\n",
    "    return a * b\n",
    "\n",
    "# This will be a tool\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Adds a and b.\n",
    "\n",
    "    Args:\n",
    "        a: first int\n",
    "        b: second int\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "\n",
    "def divide(a: int, b: int) -> float:\n",
    "    \"\"\"Divide a by b.\n",
    "\n",
    "    Args:\n",
    "        a: first int\n",
    "        b: second int\n",
    "    \"\"\"\n",
    "    return a / b\n",
    "\n",
    "tools = [add, multiply, divide]\n",
    "llm = ChatGroq(model=os.getenv(\"GROQ_MODEL_NAME\"))\n",
    "llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import MessagesState\n",
    "from langgraph.graph import START, StateGraph\n",
    "from langgraph.prebuilt import tools_condition, ToolNode\n",
    "\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "# System message\n",
    "sys_msg = SystemMessage(content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\")\n",
    "\n",
    "# Node\n",
    "def assistant(state: MessagesState):\n",
    "   return {\"messages\": [llm_with_tools.invoke([sys_msg] + state[\"messages\"])]}\n",
    "\n",
    "# Graph\n",
    "builder = StateGraph(MessagesState)\n",
    "\n",
    "# Define nodes: these do the work\n",
    "builder.add_node(\"assistant\", assistant)\n",
    "builder.add_node(\"tools\", ToolNode(tools))\n",
    "\n",
    "# Define edges: these determine the control flow\n",
    "builder.add_edge(START, \"assistant\")\n",
    "builder.add_conditional_edges(\n",
    "    \"assistant\",\n",
    "    # If the latest message (result) from assistant is a tool call -> tools_condition routes to tools\n",
    "    # If the latest message (result) from assistant is a not a tool call -> tools_condition routes to END\n",
    "    tools_condition,\n",
    ")\n",
    "builder.add_edge(\"tools\", \"assistant\")\n",
    "\n",
    "memory = MemorySaver()\n",
    "graph = builder.compile(interrupt_before=[\"assistant\"], checkpointer=memory)\n",
    "\n",
    "# Show\n",
    "display(Image(graph.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Multiply 2 and 3\n"
     ]
    }
   ],
   "source": [
    "# Input\n",
    "initial_input = {\"messages\": \"Multiply 2 and 3\"}\n",
    "\n",
    "# Thread\n",
    "thread = {\"configurable\": {\"thread_id\":\"2\"}}\n",
    "\n",
    "# Run the graph until the first interptution\n",
    "for envent in graph.stream(initial_input , thread , stream_mode=\"values\"):\n",
    "    envent[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StateSnapshot(values={'messages': [HumanMessage(content='Multiply 2 and 3', additional_kwargs={}, response_metadata={}, id='320f7854-04f6-4a79-8b3f-d7586480a52e')]}, next=('assistant',), config={'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1f02ff65-0892-6e08-8000-d74caf20de79'}}, metadata={'source': 'loop', 'writes': None, 'step': 0, 'parents': {}, 'thread_id': '2'}, created_at='2025-05-13T12:32:20.811309+00:00', parent_config={'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1f02ff65-0890-6da6-bfff-c06a03b75df0'}}, tasks=(PregelTask(id='5711b2aa-9e22-cfc7-c0fb-3c82dc2cf232', name='assistant', path=('__pregel_pull', 'assistant'), error=None, interrupts=(), state=None, result=None),), interrupts=())"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = graph.get_state(thread)\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'configurable': {'thread_id': '2',\n",
       "  'checkpoint_ns': '',\n",
       "  'checkpoint_id': '1f02ff65-094c-6f7e-8001-275cddb6ad64'}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.update_state(\n",
    "    thread,\n",
    "    {\"messages\": [HumanMessage(content=\"No, actually multiply 3 and 3!\")]},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Multiply 2 and 3\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "No, actually multiply 3 and 3!\n"
     ]
    }
   ],
   "source": [
    "new_state = graph.get_state(thread).values\n",
    "for m in new_state['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "No, actually multiply 3 and 3!\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  multiply (call_4p20)\n",
      " Call ID: call_4p20\n",
      "  Args:\n",
      "    a: 3\n",
      "    b: 3\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: multiply\n",
      "\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "for event in graph.stream(None, thread, stream_mode=\"values\"):\n",
    "    event['messages'][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: multiply\n",
      "\n",
      "9\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The result of multiplying 3 and 3 is 9.\n"
     ]
    }
   ],
   "source": [
    "for event in graph.stream(None, thread, stream_mode=\"values\"):\n",
    "    event['messages'][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the URL of the local development server\n",
    "from langgraph_sdk import get_client\n",
    "client = get_client(url=\"http://127.0.0.1:2024\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving new event of type: metadata...\n",
      "--------------------------------------------------\n",
      "Receiving new event of type: values...\n",
      "{'content': 'Multiply 2 and 3', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': 'b7d0ba71-088a-40d0-9c84-2603eb2bab30', 'example': False}\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "initial_input = {\"messages\": \"Multiply 2 and 3\"}\n",
    "thread = await client.threads.create()\n",
    "async for chunk in client.runs.stream(\n",
    "    thread[\"thread_id\"],\n",
    "    \"agent\",\n",
    "    input=initial_input,\n",
    "    stream_mode=\"values\",\n",
    "    interrupt_before=[\"assistant\"],\n",
    "):\n",
    "    print(f\"Receiving new event of type: {chunk.event}...\")\n",
    "    messages = chunk.data.get('messages', [])\n",
    "    if messages:\n",
    "        print(messages[-1])\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'values': {'messages': [{'content': 'Multiply 2 and 3',\n",
       "    'additional_kwargs': {},\n",
       "    'response_metadata': {},\n",
       "    'type': 'human',\n",
       "    'name': None,\n",
       "    'id': 'b7d0ba71-088a-40d0-9c84-2603eb2bab30',\n",
       "    'example': False}]},\n",
       " 'next': ['assistant'],\n",
       " 'tasks': [{'id': 'c850cbb3-32d9-02fa-fc48-abcf03171a97',\n",
       "   'name': 'assistant',\n",
       "   'path': ['__pregel_pull', 'assistant'],\n",
       "   'error': None,\n",
       "   'interrupts': [],\n",
       "   'checkpoint': None,\n",
       "   'state': None,\n",
       "   'result': None}],\n",
       " 'metadata': {'user-agent': 'langgraph-sdk-py/0.1.69',\n",
       "  'x-request-id': '68c63483-5d1f-44c8-859c-d98449245a84',\n",
       "  'langgraph_auth_user': None,\n",
       "  'langgraph_auth_user_id': '',\n",
       "  'langgraph_auth_permissions': [],\n",
       "  'langgraph_request_id': '68c63483-5d1f-44c8-859c-d98449245a84',\n",
       "  'graph_id': 'agent',\n",
       "  'assistant_id': 'fe096781-5601-53d2-b2f6-0d3403f7e9ca',\n",
       "  'user_id': '',\n",
       "  'created_by': 'system',\n",
       "  'run_attempt': 1,\n",
       "  'langgraph_version': '0.4.3',\n",
       "  'langgraph_plan': 'developer',\n",
       "  'langgraph_host': 'self-hosted',\n",
       "  'langgraph_api_url': 'http://127.0.0.1:2024',\n",
       "  'run_id': '1f02ff65-0f60-63ca-987b-d3d448da2c42',\n",
       "  'thread_id': 'fbdb7496-cb04-46f5-809b-1df264b20b4d',\n",
       "  'source': 'loop',\n",
       "  'writes': None,\n",
       "  'step': 0,\n",
       "  'parents': {}},\n",
       " 'created_at': '2025-05-13T12:32:22.158390+00:00',\n",
       " 'checkpoint': {'checkpoint_id': '1f02ff65-156b-6a8a-8000-ea5ecbcf8276',\n",
       "  'thread_id': 'fbdb7496-cb04-46f5-809b-1df264b20b4d',\n",
       "  'checkpoint_ns': ''},\n",
       " 'parent_checkpoint': {'checkpoint_id': '1f02ff65-1564-67bc-bfff-0a5ce43a6ac4',\n",
       "  'thread_id': 'fbdb7496-cb04-46f5-809b-1df264b20b4d',\n",
       "  'checkpoint_ns': ''},\n",
       " 'checkpoint_id': '1f02ff65-156b-6a8a-8000-ea5ecbcf8276',\n",
       " 'parent_checkpoint_id': '1f02ff65-1564-67bc-bfff-0a5ce43a6ac4'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_state = await client.threads.get_state(thread['thread_id'])\n",
    "current_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content': 'Multiply 2 and 3',\n",
       " 'additional_kwargs': {},\n",
       " 'response_metadata': {},\n",
       " 'type': 'human',\n",
       " 'name': None,\n",
       " 'id': 'b7d0ba71-088a-40d0-9c84-2603eb2bab30',\n",
       " 'example': False}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_message = current_state['values']['messages'][-1]\n",
    "last_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content': 'No, actually multiply 3 and 3!',\n",
       " 'additional_kwargs': {},\n",
       " 'response_metadata': {},\n",
       " 'type': 'human',\n",
       " 'name': None,\n",
       " 'id': 'b7d0ba71-088a-40d0-9c84-2603eb2bab30',\n",
       " 'example': False}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_message['content'] = \"No, actually multiply 3 and 3!\"\n",
    "last_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content': 'No, actually multiply 3 and 3!',\n",
       " 'additional_kwargs': {},\n",
       " 'response_metadata': {},\n",
       " 'type': 'human',\n",
       " 'name': None,\n",
       " 'id': 'b7d0ba71-088a-40d0-9c84-2603eb2bab30',\n",
       " 'example': False}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'checkpoint': {'thread_id': 'fbdb7496-cb04-46f5-809b-1df264b20b4d',\n",
       "  'checkpoint_ns': '',\n",
       "  'checkpoint_id': '1f02ff65-15d5-60de-8001-6d6f55d0542a'},\n",
       " 'configurable': {'thread_id': 'fbdb7496-cb04-46f5-809b-1df264b20b4d',\n",
       "  'checkpoint_ns': '',\n",
       "  'checkpoint_id': '1f02ff65-15d5-60de-8001-6d6f55d0542a'},\n",
       " 'checkpoint_id': '1f02ff65-15d5-60de-8001-6d6f55d0542a'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await client.threads.update_state(thread['thread_id'], {\"messages\": last_message})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving new event of type: metadata...\n",
      "--------------------------------------------------\n",
      "Receiving new event of type: values...\n",
      "{'content': 'No, actually multiply 3 and 3!', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': 'b7d0ba71-088a-40d0-9c84-2603eb2bab30', 'example': False}\n",
      "--------------------------------------------------\n",
      "Receiving new event of type: values...\n",
      "{'content': '', 'additional_kwargs': {'tool_calls': [{'id': 'call_xy2y', 'function': {'arguments': '{\"a\": 3, \"b\": 3}', 'name': 'multiply'}, 'type': 'function'}]}, 'response_metadata': {'token_usage': {'completion_tokens': 19, 'prompt_tokens': 378, 'total_tokens': 397, 'completion_time': 0.069090909, 'prompt_time': 0.02435025, 'queue_time': 0.06160526999999999, 'total_time': 0.093441159}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_2ddfbb0da0', 'finish_reason': 'tool_calls', 'logprobs': None}, 'type': 'ai', 'name': None, 'id': 'run--90246b6c-bb00-426b-95bc-8a163a600711-0', 'example': False, 'tool_calls': [{'name': 'multiply', 'args': {'a': 3, 'b': 3}, 'id': 'call_xy2y', 'type': 'tool_call'}], 'invalid_tool_calls': [], 'usage_metadata': {'input_tokens': 378, 'output_tokens': 19, 'total_tokens': 397}}\n",
      "--------------------------------------------------\n",
      "Receiving new event of type: values...\n",
      "{'content': '9', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': 'multiply', 'id': 'a6471cc8-f5d3-4f00-b545-fe50742bf3d3', 'tool_call_id': 'call_xy2y', 'artifact': None, 'status': 'success'}\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "async for chunk in client.runs.stream(\n",
    "    thread[\"thread_id\"],\n",
    "    assistant_id=\"agent\",\n",
    "    input=None,\n",
    "    stream_mode=\"values\",\n",
    "    interrupt_before=[\"assistant\"],\n",
    "):\n",
    "    print(f\"Receiving new event of type: {chunk.event}...\")\n",
    "    messages = chunk.data.get('messages', [])\n",
    "    if messages:\n",
    "        print(messages[-1])\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving new event of type: metadata...\n",
      "--------------------------------------------------\n",
      "Receiving new event of type: values...\n",
      "{'content': '9', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': 'multiply', 'id': 'a6471cc8-f5d3-4f00-b545-fe50742bf3d3', 'tool_call_id': 'call_xy2y', 'artifact': None, 'status': 'success'}\n",
      "--------------------------------------------------\n",
      "Receiving new event of type: values...\n",
      "{'content': 'The result of multiplying 3 and 3 is 9.', 'additional_kwargs': {}, 'response_metadata': {'token_usage': {'completion_tokens': 14, 'prompt_tokens': 408, 'total_tokens': 422, 'completion_time': 0.050909091, 'prompt_time': 0.026555983, 'queue_time': 0.05911386700000001, 'total_time': 0.077465074}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_2ddfbb0da0', 'finish_reason': 'stop', 'logprobs': None}, 'type': 'ai', 'name': None, 'id': 'run--de58d457-d41d-445f-9280-ed4be24144d7-0', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': {'input_tokens': 408, 'output_tokens': 14, 'total_tokens': 422}}\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "async for chunk in client.runs.stream(\n",
    "    thread[\"thread_id\"],\n",
    "    assistant_id=\"agent\",\n",
    "    input=None,\n",
    "    stream_mode=\"values\",\n",
    "    interrupt_before=[\"assistant\"],\n",
    "):\n",
    "    print(f\"Receiving new event of type: {chunk.event}...\")\n",
    "    messages = chunk.data.get('messages', [])\n",
    "    if messages:\n",
    "        print(messages[-1])\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Awaiting user input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to reach https://mermaid.ink/ API while trying to render your graph after 1 retries. To resolve this issue:\n1. Check your internet connection and try again\n2. Try with higher retry settings: `draw_mermaid_png(..., max_retries=5, retry_delay=2.0)`\n3. Use the Pyppeteer rendering method which will render your graph locally in a browser: `draw_mermaid_png(..., draw_method=MermaidDrawMethod.PYPPETEER)`",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTimeoutError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/GEN_AI/langgraph-env/lib/python3.11/site-packages/urllib3/connectionpool.py:534\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m534\u001b[39m     response = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/GEN_AI/langgraph-env/lib/python3.11/site-packages/urllib3/connection.py:516\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    515\u001b[39m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m516\u001b[39m httplib_response = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py:1390\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1389\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1390\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1391\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py:325\u001b[39m, in \u001b[36mHTTPResponse.begin\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    324\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m325\u001b[39m     version, status, reason = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m status != CONTINUE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py:286\u001b[39m, in \u001b[36mHTTPResponse._read_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m     line = \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m.fp.readline(_MAXLINE + \u001b[32m1\u001b[39m), \u001b[33m\"\u001b[39m\u001b[33miso-8859-1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    287\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) > _MAXLINE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socket.py:706\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m706\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    707\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/ssl.py:1314\u001b[39m, in \u001b[36mSSLSocket.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes, flags)\u001b[39m\n\u001b[32m   1311\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1312\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1313\u001b[39m           \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1314\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1315\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/ssl.py:1166\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1165\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1166\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1167\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mTimeoutError\u001b[39m: The read operation timed out",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mReadTimeoutError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/GEN_AI/langgraph-env/lib/python3.11/site-packages/requests/adapters.py:667\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    666\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m667\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    668\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    669\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    679\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    681\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/GEN_AI/langgraph-env/lib/python3.11/site-packages/urllib3/connectionpool.py:841\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    839\u001b[39m     new_e = ProtocolError(\u001b[33m\"\u001b[39m\u001b[33mConnection aborted.\u001b[39m\u001b[33m\"\u001b[39m, new_e)\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m retries = \u001b[43mretries\u001b[49m\u001b[43m.\u001b[49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    842\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[43m=\u001b[49m\u001b[43msys\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    844\u001b[39m retries.sleep()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/GEN_AI/langgraph-env/lib/python3.11/site-packages/urllib3/util/retry.py:474\u001b[39m, in \u001b[36mRetry.increment\u001b[39m\u001b[34m(self, method, url, response, error, _pool, _stacktrace)\u001b[39m\n\u001b[32m    473\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._is_method_retryable(method):\n\u001b[32m--> \u001b[39m\u001b[32m474\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    475\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/GEN_AI/langgraph-env/lib/python3.11/site-packages/urllib3/util/util.py:39\u001b[39m, in \u001b[36mreraise\u001b[39m\u001b[34m(tp, value, tb)\u001b[39m\n\u001b[32m     38\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m value.with_traceback(tb)\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m value\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/GEN_AI/langgraph-env/lib/python3.11/site-packages/urllib3/connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/GEN_AI/langgraph-env/lib/python3.11/site-packages/urllib3/connectionpool.py:536\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m536\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_timeout\u001b[49m\u001b[43m(\u001b[49m\u001b[43merr\u001b[49m\u001b[43m=\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mread_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    537\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/GEN_AI/langgraph-env/lib/python3.11/site-packages/urllib3/connectionpool.py:367\u001b[39m, in \u001b[36mHTTPConnectionPool._raise_timeout\u001b[39m\u001b[34m(self, err, url, timeout_value)\u001b[39m\n\u001b[32m    366\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(err, SocketTimeout):\n\u001b[32m--> \u001b[39m\u001b[32m367\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ReadTimeoutError(\n\u001b[32m    368\u001b[39m         \u001b[38;5;28mself\u001b[39m, url, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRead timed out. (read timeout=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimeout_value\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    369\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m    371\u001b[39m \u001b[38;5;66;03m# See the above comment about EAGAIN in Python 3.\u001b[39;00m\n",
      "\u001b[31mReadTimeoutError\u001b[39m: HTTPSConnectionPool(host='mermaid.ink', port=443): Read timed out. (read timeout=10)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mReadTimeout\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/GEN_AI/langgraph-env/lib/python3.11/site-packages/langchain_core/runnables/graph_mermaid.py:430\u001b[39m, in \u001b[36m_render_mermaid_using_api\u001b[39m\u001b[34m(mermaid_syntax, output_file_path, background_color, file_type, max_retries, retry_delay)\u001b[39m\n\u001b[32m    429\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m430\u001b[39m     response = \u001b[43mrequests\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    431\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m response.status_code == requests.codes.ok:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/GEN_AI/langgraph-env/lib/python3.11/site-packages/requests/api.py:73\u001b[39m, in \u001b[36mget\u001b[39m\u001b[34m(url, params, **kwargs)\u001b[39m\n\u001b[32m     63\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[32m     64\u001b[39m \n\u001b[32m     65\u001b[39m \u001b[33;03m:param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     70\u001b[39m \u001b[33;03m:rtype: requests.Response\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mget\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/GEN_AI/langgraph-env/lib/python3.11/site-packages/requests/api.py:59\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(method, url, **kwargs)\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sessions.Session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/GEN_AI/langgraph-env/lib/python3.11/site-packages/requests/sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/GEN_AI/langgraph-env/lib/python3.11/site-packages/requests/sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/GEN_AI/langgraph-env/lib/python3.11/site-packages/requests/adapters.py:713\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    712\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, ReadTimeoutError):\n\u001b[32m--> \u001b[39m\u001b[32m713\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ReadTimeout(e, request=request)\n\u001b[32m    714\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, _InvalidHeader):\n",
      "\u001b[31mReadTimeout\u001b[39m: HTTPSConnectionPool(host='mermaid.ink', port=443): Read timed out. (read timeout=10)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     31\u001b[39m memory = MemorySaver()\n\u001b[32m     32\u001b[39m graph = builder.compile(interrupt_before=[\u001b[33m\"\u001b[39m\u001b[33mhuman_feedback\u001b[39m\u001b[33m\"\u001b[39m], checkpointer=memory)\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m display(Image(\u001b[43mgraph\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdraw_mermaid_png\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/GEN_AI/langgraph-env/lib/python3.11/site-packages/langchain_core/runnables/graph.py:685\u001b[39m, in \u001b[36mGraph.draw_mermaid_png\u001b[39m\u001b[34m(self, curve_style, node_colors, wrap_label_n_words, output_file_path, draw_method, background_color, padding, max_retries, retry_delay, frontmatter_config)\u001b[39m\n\u001b[32m    677\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrunnables\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgraph_mermaid\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m draw_mermaid_png\n\u001b[32m    679\u001b[39m mermaid_syntax = \u001b[38;5;28mself\u001b[39m.draw_mermaid(\n\u001b[32m    680\u001b[39m     curve_style=curve_style,\n\u001b[32m    681\u001b[39m     node_colors=node_colors,\n\u001b[32m    682\u001b[39m     wrap_label_n_words=wrap_label_n_words,\n\u001b[32m    683\u001b[39m     frontmatter_config=frontmatter_config,\n\u001b[32m    684\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m685\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw_mermaid_png\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    686\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmermaid_syntax\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmermaid_syntax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    687\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    688\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdraw_method\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdraw_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    689\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    690\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    691\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    692\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretry_delay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_delay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    693\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/GEN_AI/langgraph-env/lib/python3.11/site-packages/langchain_core/runnables/graph_mermaid.py:293\u001b[39m, in \u001b[36mdraw_mermaid_png\u001b[39m\u001b[34m(mermaid_syntax, output_file_path, draw_method, background_color, padding, max_retries, retry_delay)\u001b[39m\n\u001b[32m    287\u001b[39m     img_bytes = asyncio.run(\n\u001b[32m    288\u001b[39m         _render_mermaid_using_pyppeteer(\n\u001b[32m    289\u001b[39m             mermaid_syntax, output_file_path, background_color, padding\n\u001b[32m    290\u001b[39m         )\n\u001b[32m    291\u001b[39m     )\n\u001b[32m    292\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m draw_method == MermaidDrawMethod.API:\n\u001b[32m--> \u001b[39m\u001b[32m293\u001b[39m     img_bytes = \u001b[43m_render_mermaid_using_api\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    294\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmermaid_syntax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_delay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_delay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    301\u001b[39m     supported_methods = \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join([m.value \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m MermaidDrawMethod])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/GEN_AI/langgraph-env/lib/python3.11/site-packages/langchain_core/runnables/graph_mermaid.py:462\u001b[39m, in \u001b[36m_render_mermaid_using_api\u001b[39m\u001b[34m(mermaid_syntax, output_file_path, background_color, file_type, max_retries, retry_delay)\u001b[39m\n\u001b[32m    457\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    458\u001b[39m             msg = (\n\u001b[32m    459\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mFailed to reach https://mermaid.ink/ API while trying to render \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    460\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33myour graph after \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_retries\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m retries. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    461\u001b[39m             ) + error_msg_suffix\n\u001b[32m--> \u001b[39m\u001b[32m462\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    464\u001b[39m \u001b[38;5;66;03m# This should not be reached, but just in case\u001b[39;00m\n\u001b[32m    465\u001b[39m msg = (\n\u001b[32m    466\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mFailed to reach https://mermaid.ink/ API while trying to render \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    467\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33myour graph after \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_retries\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m retries. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    468\u001b[39m ) + error_msg_suffix\n",
      "\u001b[31mValueError\u001b[39m: Failed to reach https://mermaid.ink/ API while trying to render your graph after 1 retries. To resolve this issue:\n1. Check your internet connection and try again\n2. Try with higher retry settings: `draw_mermaid_png(..., max_retries=5, retry_delay=2.0)`\n3. Use the Pyppeteer rendering method which will render your graph locally in a browser: `draw_mermaid_png(..., draw_method=MermaidDrawMethod.PYPPETEER)`"
     ]
    }
   ],
   "source": [
    "# System message\n",
    "sys_msg = SystemMessage(content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\")\n",
    "\n",
    "# no-op node that should be interrupted on\n",
    "def human_feedback(state: MessagesState):\n",
    "    pass\n",
    "\n",
    "# Assistant node\n",
    "def assistant(state: MessagesState):\n",
    "   return {\"messages\": [llm_with_tools.invoke([sys_msg] + state[\"messages\"])]}\n",
    "\n",
    "# Graph\n",
    "builder = StateGraph(MessagesState)\n",
    "\n",
    "# Define nodes: these do the work\n",
    "builder.add_node(\"assistant\", assistant)\n",
    "builder.add_node(\"tools\", ToolNode(tools))\n",
    "builder.add_node(\"human_feedback\", human_feedback)\n",
    "\n",
    "# Define edges: these determine the control flow\n",
    "builder.add_edge(START, \"human_feedback\")\n",
    "builder.add_edge(\"human_feedback\", \"assistant\")\n",
    "builder.add_conditional_edges(\n",
    "    \"assistant\",\n",
    "    # If the latest message (result) from assistant is a tool call -> tools_condition routes to tools\n",
    "    # If the latest message (result) from assistant is a not a tool call -> tools_condition routes to END\n",
    "    tools_condition,\n",
    ")\n",
    "builder.add_edge(\"tools\", \"human_feedback\")\n",
    "\n",
    "memory = MemorySaver()\n",
    "graph = builder.compile(interrupt_before=[\"human_feedback\"], checkpointer=memory)\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Multiply 2 and 3\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "no, multiply 3 and 3\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  multiply (call_80aq)\n",
      " Call ID: call_80aq\n",
      "  Args:\n",
      "    a: 3\n",
      "    b: 3\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: multiply\n",
      "\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "# Input\n",
    "initial_input = {\"messages\": \"Multiply 2 and 3\"}\n",
    "\n",
    "# Thread\n",
    "thread = {\"configurable\": {\"thread_id\": \"5\"}}\n",
    "\n",
    "# Run the graph until the first interruption\n",
    "for event in graph.stream(initial_input, thread, stream_mode=\"values\"):\n",
    "    event[\"messages\"][-1].pretty_print()\n",
    "    \n",
    "# Get user input\n",
    "user_input = input(\"Tell me how you want to update the state: \")\n",
    "\n",
    "# We now update the state as if we are the human_feedback node\n",
    "graph.update_state(thread, {\"messages\": user_input}, as_node=\"human_feedback\")\n",
    "\n",
    "# Continue the graph execution\n",
    "for event in graph.stream(None, thread, stream_mode=\"values\"):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: multiply\n",
      "\n",
      "9\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The result of multiplying 3 and 3 is 9.\n"
     ]
    }
   ],
   "source": [
    "# Continue the graph execution\n",
    "for event in graph.stream(None, thread, stream_mode=\"values\"):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langgraph-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
