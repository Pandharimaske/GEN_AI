{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <B> Chatbot with Collection Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install -U langchain_openai langgraph trustcall langchain_core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a collection schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Memory(BaseModel):\n",
    "    \"\"\"Represents a single memory entry.\"\"\"\n",
    "    content: str = Field(description=\"The main content of the memory. For example: User expressed interest in learning about French.\")\n",
    "\n",
    "class MemoryCollection(BaseModel):\n",
    "    \"\"\"Extracts and stores a collection of user memory entries.\"\"\"\n",
    "    memories: list[Memory] = Field(description=\"A list of memories about the user.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Memory(content='User name is Pandhari'),\n",
       " Memory(content='Pandhari likes to bike')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# Initialize the model\n",
    "model = ChatGroq(model=os.getenv(\"GROQ_MODEL_NAME\") , temperature=0)\n",
    "\n",
    "# Bind schema to model\n",
    "model_with_structure = model.with_structured_output(MemoryCollection)\n",
    "\n",
    "# Invoke the model to produce structured output that matches the schema\n",
    "memory_collection = model_with_structure.invoke([HumanMessage(\"My name is Pandhari. I like to bike\")])\n",
    "memory_collection.memories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content': 'User name is Pandhari'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory_collection.memories[0].model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "\n",
    "# Initialize the in-memory store\n",
    "in_memory_store = InMemoryStore()\n",
    "\n",
    "# Namespace for the memory to save\n",
    "user_id = \"1\"\n",
    "namespace_for_memory = (user_id , \"memories\")\n",
    "\n",
    "# Save a memory to namespace as key and value\n",
    "key = str(uuid.uuid4())\n",
    "value = memory_collection.memories[0].model_dump()\n",
    "in_memory_store.put(namespace_for_memory , key , value)\n",
    "\n",
    "key = str(uuid.uuid4())\n",
    "value = memory_collection.memories[1].model_dump()\n",
    "in_memory_store.put(namespace_for_memory , key , value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'namespace': ['1', 'memories'], 'key': '2e9ecbcd-d405-4939-90a6-6311aeca47c5', 'value': {'content': 'User name is Pandhari'}, 'created_at': '2025-05-22T16:05:29.333717+00:00', 'updated_at': '2025-05-22T16:05:29.333719+00:00', 'score': None}\n",
      "{'namespace': ['1', 'memories'], 'key': '29f76075-faff-439e-9b18-74747b604cb5', 'value': {'content': 'Pandhari likes to bike'}, 'created_at': '2025-05-22T16:05:29.333779+00:00', 'updated_at': '2025-05-22T16:05:29.333780+00:00', 'score': None}\n"
     ]
    }
   ],
   "source": [
    "# Search\n",
    "for m in in_memory_store.search(namespace_for_memory):\n",
    "    print(m.dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating collection schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trustcall import create_extractor\n",
    "\n",
    "# Create the extractor\n",
    "trustcall_extractor = create_extractor(\n",
    "    model,\n",
    "    tools=[Memory],\n",
    "    tool_choice=\"Memory\",\n",
    "    enable_inserts=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "# Instruction\n",
    "instruction = \"\"\"Extract memories from the following conversation:\"\"\"\n",
    "\n",
    "# Conversation\n",
    "conversation = [HumanMessage(content=\"Hi, I'm Pandhari.\"), \n",
    "                AIMessage(content=\"Nice to meet you, Pandhari.\"), \n",
    "                HumanMessage(content=\"This morning I had a nice bike ride in San Francisco.\")]\n",
    "\n",
    "# Invoke the extractor\n",
    "result = trustcall_extractor.invoke({\"messages\": [SystemMessage(content=instruction)] + conversation})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  Memory (call_x5z8)\n",
      " Call ID: call_x5z8\n",
      "  Args:\n",
      "    content: User had a bike ride in San Francisco this morning\n"
     ]
    }
   ],
   "source": [
    "# Messages contain the tool calls\n",
    "for m in result[\"messages\"]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='User had a bike ride in San Francisco this morning'\n"
     ]
    }
   ],
   "source": [
    "# Responses contain the memories that adhere to the schema\n",
    "for m in result[\"responses\"]: \n",
    "    print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'call_x5z8'}\n"
     ]
    }
   ],
   "source": [
    "# Metadata contains the tool call  \n",
    "for m in result[\"response_metadata\"]: \n",
    "    print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('0',\n",
       "  'Memory',\n",
       "  {'content': 'User had a bike ride in San Francisco this morning'})]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Update the conversation\n",
    "updated_conversation = [AIMessage(content=\"That's great, did you do after?\"), \n",
    "                        HumanMessage(content=\"I went to Tartine and ate a croissant.\"),                        \n",
    "                        AIMessage(content=\"What else is on your mind?\"),\n",
    "                        HumanMessage(content=\"I was thinking about my Japan, and going back this winter!\"),]\n",
    "\n",
    "# Update the instruction\n",
    "system_msg = \"\"\"Update existing memories and create new ones based on the following conversation:\"\"\"\n",
    "\n",
    "# We'll save existing memories, giving them an ID, key (tool name), and value\n",
    "tool_name = \"Memory\"\n",
    "existing_memories = [(str(i), tool_name, memory.model_dump()) for i, memory in enumerate(result[\"responses\"])] if result[\"responses\"] else None\n",
    "existing_memories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke the extractor with our updated conversation and existing memories\n",
    "result = trustcall_extractor.invoke({\"messages\": updated_conversation, \n",
    "                                     \"existing\": existing_memories})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  Memory (call_50v9)\n",
      " Call ID: call_50v9\n",
      "  Args:\n",
      "    content: User expressed interest in traveling to Japan this winter and had a previous experience there.\n",
      "  Memory (call_jvz2)\n",
      " Call ID: call_jvz2\n",
      "  Args:\n",
      "    content: User had a bike ride in San Francisco this morning\n",
      "    -: {'content': 'User is planning to travel to Japan this winter'}\n"
     ]
    }
   ],
   "source": [
    "# Messages from the model indicate two tool calls were made\n",
    "for m in result[\"messages\"]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='User expressed interest in traveling to Japan this winter and had a previous experience there.'\n",
      "content='User had a bike ride in San Francisco this morning'\n"
     ]
    }
   ],
   "source": [
    "# Responses contain the memories that adhere to the schema\n",
    "for m in result[\"responses\"]: \n",
    "    print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'call_50v9'}\n",
      "{'id': 'call_jvz2', 'json_doc_id': '0'}\n"
     ]
    }
   ],
   "source": [
    "# Metadata contains the tool call  \n",
    "for m in result[\"response_metadata\"]: \n",
    "    print(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chatbot with collection schema updating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "import uuid\n",
    "\n",
    "from langgraph.graph import StateGraph, MessagesState, START, END\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "from langchain_core.messages import merge_message_runs\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.runnables.config import RunnableConfig\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.store.base import BaseStore\n",
    "\n",
    "# Initialize the model\n",
    "model = ChatGroq(model=os.getenv(\"GROQ_MODEL_NAME\"), temperature=0)\n",
    "\n",
    "# Memory schema\n",
    "class Memory(BaseModel):\n",
    "    \"\"\"Represents a single memory entry.\"\"\"\n",
    "    content: str = Field(description=\"The main content of the memory. For example: User expressed interest in learning about French.\")\n",
    "\n",
    "# Create the Trustcall extractor\n",
    "trustcall_extractor = create_extractor(\n",
    "    model,\n",
    "    tools=[Memory],\n",
    "    tool_choice=\"Memory\",\n",
    "    # This allows the extractor to insert new memories\n",
    "    enable_inserts=True,\n",
    ")\n",
    "\n",
    "# Chatbot instruction\n",
    "MODEL_SYSTEM_MESSAGE = \"\"\"You are a helpful chatbot. You are designed to be a companion to a user. \n",
    "\n",
    "You have a long term memory which keeps track of information you learn about the user over time.\n",
    "\n",
    "Current Memory (may include updated memories from this conversation): \n",
    "\n",
    "{memory}\"\"\"\n",
    "\n",
    "# Trustcall instruction\n",
    "TRUSTCALL_INSTRUCTION = \"\"\"Reflect on following interaction. \n",
    "\n",
    "Use the provided tools to retain any necessary memories about the user. \n",
    "\n",
    "Use parallel tool calling to handle updates and insertions simultaneously:\"\"\"\n",
    "\n",
    "def call_model(state: MessagesState, config: RunnableConfig, store: BaseStore):\n",
    "\n",
    "    \"\"\"Load memories from the store and use them to personalize the chatbot's response.\"\"\"\n",
    "    \n",
    "    # Get the user ID from the config\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "\n",
    "    # Retrieve memory from the store\n",
    "    namespace = (\"memories\", user_id)\n",
    "    memories = store.search(namespace)\n",
    "\n",
    "    # Format the memories for the system prompt\n",
    "    info = \"\\n\".join(f\"- {mem.value['content']}\" for mem in memories)\n",
    "    system_msg = MODEL_SYSTEM_MESSAGE.format(memory=info)\n",
    "\n",
    "    # Respond using memory as well as the chat history\n",
    "    response = model.invoke([SystemMessage(content=system_msg)]+state[\"messages\"])\n",
    "\n",
    "    return {\"messages\": response}\n",
    "\n",
    "def write_memory(state: MessagesState, config: RunnableConfig, store: BaseStore):\n",
    "\n",
    "    \"\"\"Reflect on the chat history and update the memory collection.\"\"\"\n",
    "    \n",
    "    # Get the user ID from the config\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "\n",
    "    # Define the namespace for the memories\n",
    "    namespace = (\"memories\", user_id)\n",
    "\n",
    "    # Retrieve the most recent memories for context\n",
    "    existing_items = store.search(namespace)\n",
    "\n",
    "    # Format the existing memories for the Trustcall extractor\n",
    "    tool_name = \"Memory\"\n",
    "    existing_memories = ([(existing_item.key, tool_name, existing_item.value)\n",
    "                          for existing_item in existing_items]\n",
    "                          if existing_items\n",
    "                          else None\n",
    "                        )\n",
    "\n",
    "    # Merge the chat history and the instruction\n",
    "    updated_messages=list(merge_message_runs(messages=[SystemMessage(content=TRUSTCALL_INSTRUCTION)] + state[\"messages\"]))\n",
    "\n",
    "    # Invoke the extractor\n",
    "    result = trustcall_extractor.invoke({\"messages\": updated_messages, \n",
    "                                        \"existing\": existing_memories})\n",
    "\n",
    "    # Save the memories from Trustcall to the store\n",
    "    for r, rmeta in zip(result[\"responses\"], result[\"response_metadata\"]):\n",
    "        store.put(namespace,\n",
    "                  rmeta.get(\"json_doc_id\", str(uuid.uuid4())),\n",
    "                  r.model_dump(mode=\"json\"),\n",
    "            )\n",
    "\n",
    "# Define the graph\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"call_model\", call_model)\n",
    "builder.add_node(\"write_memory\", write_memory)\n",
    "builder.add_edge(START, \"call_model\")\n",
    "builder.add_edge(\"call_model\", \"write_memory\")\n",
    "builder.add_edge(\"write_memory\", END)\n",
    "\n",
    "# Store for long-term (across-thread) memory\n",
    "across_thread_memory = InMemoryStore()\n",
    "\n",
    "# Checkpointer for short-term (within-thread) memory\n",
    "within_thread_memory = MemorySaver()\n",
    "\n",
    "# Compile the graph with the checkpointer fir and store\n",
    "graph = builder.compile(checkpointer=within_thread_memory, store=across_thread_memory)\n",
    "\n",
    "# View\n",
    "display(Image(graph.get_graph(xray=1).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hi, my name is Pandhari\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello Pandhari, it's nice to meet you. I'll make sure to remember your name for our future conversations. Is there something I can help you with or would you like to chat and get to know each other? \n",
      "\n",
      "Current Memory: \n",
      "- Your name is Pandhari\n"
     ]
    }
   ],
   "source": [
    "# We supply a thread ID for short-term (within-thread) memory\n",
    "# We supply a user ID for long-term (across-thread) memory \n",
    "config = {\"configurable\": {\"thread_id\": \"1\", \"user_id\": \"1\"}}\n",
    "\n",
    "# User input \n",
    "input_messages = [HumanMessage(content=\"Hi, my name is Pandhari\")]\n",
    "\n",
    "# Run the graph\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "I like to bike around San Francisco\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "San Francisco is a beautiful city with a lot of great bike routes. The hills can be a challenge, but the views of the Golden Gate Bridge, the Bay, and the city skyline are definitely worth it. Do you have a favorite biking spot or route in San Francisco, Pandhari?\n",
      "\n",
      "Current Memory: \n",
      "- Your name is Pandhari\n",
      "- You like to bike around San Francisco\n"
     ]
    }
   ],
   "source": [
    "# User input \n",
    "input_messages = [HumanMessage(content=\"I like to bike around San Francisco\")]\n",
    "\n",
    "# Run the graph\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'namespace': ['memories', '1'], 'key': '543564c8-fadb-4b4d-b60b-ac91211f0cc5', 'value': {'content': 'User introduced themselves as Pandhari'}, 'created_at': '2025-05-22T16:18:22.003250+00:00', 'updated_at': '2025-05-22T16:18:22.003251+00:00', 'score': None}\n",
      "{'namespace': ['memories', '1'], 'key': 'e404615c-8049-4966-807a-e8e67158412d', 'value': {'content': 'User likes to bike around San Francisco'}, 'created_at': '2025-05-22T16:18:22.003219+00:00', 'updated_at': '2025-05-22T16:18:22.003221+00:00', 'score': None}\n",
      "{'namespace': ['memories', '1'], 'key': '71a92f8e-07b4-4c38-82d4-264b0a99f654', 'value': {'content': 'User enjoys going to bakeries'}, 'created_at': '2025-05-22T16:19:25.391314+00:00', 'updated_at': '2025-05-22T16:19:25.391315+00:00', 'score': None}\n"
     ]
    }
   ],
   "source": [
    "# Namespace for the memory to save\n",
    "user_id = \"1\"\n",
    "namespace = (\"memories\", user_id)\n",
    "memories = across_thread_memory.search(namespace)\n",
    "for m in memories:\n",
    "    print(m.dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "I also enjoy going to bakeries\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Bakeries can be a wonderful treat, especially after a long bike ride. San Francisco has a lot of great bakeries to choose from, with a wide variety of delicious pastries, bread, and other sweet treats. Have you tried any of the famous sourdough bread from some of the local bakeries, Pandhari? Or do you have a favorite type of pastry or dessert that you like to indulge in?\n",
      "\n",
      "Current Memory: \n",
      "- Your name is Pandhari\n",
      "- You like to bike around San Francisco\n",
      "- You enjoy going to bakeries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not apply patch: member 'memories' not found in {'content': 'User introduced themselves as Pandhari'}\n",
      "Could not apply patch: member 'memories' not found in {'content': 'User likes to bike around San Francisco'}\n"
     ]
    }
   ],
   "source": [
    "# User input \n",
    "input_messages = [HumanMessage(content=\"I also enjoy going to bakeries\")]\n",
    "\n",
    "# Run the graph\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What bakeries do you recommend for me?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Pandhari, I'm glad you asked. I recall that you enjoy going to bakeries. Unfortunately, I don't have any specific bakery recommendations in San Francisco yet, but I can suggest some popular ones that I can learn more about for our future conversations. \n",
      "\n",
      "Some well-known bakeries in San Francisco include Bi-Rite Bakery, Noe Valley Bakery, and Arizmendi Bakery. They offer a variety of bread, pastries, and other baked goods. If you've been to any of these or have a favorite, I'd love to hear about it so I can make more personalized recommendations in the future.\n",
      "\n",
      "By the way, have you ever biked to any of these bakeries or would you like some route suggestions?\n"
     ]
    }
   ],
   "source": [
    "# We supply a thread ID for short-term (within-thread) memory\n",
    "# We supply a user ID for long-term (across-thread) memory \n",
    "config = {\"configurable\": {\"thread_id\": \"2\", \"user_id\": \"1\"}}\n",
    "\n",
    "# User input \n",
    "input_messages = [HumanMessage(content=\"What bakeries do you recommend for me?\")]\n",
    "\n",
    "# Run the graph\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langgraph-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
