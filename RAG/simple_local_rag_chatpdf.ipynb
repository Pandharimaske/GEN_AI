{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Create and Run a local RAG pipeline from scratch**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Document/text preprocessing and embedding creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "# Get PDF document path\n",
    "pdf_path = \"human-nutrition-text.pdf\"\n",
    "\n",
    "# Download\n",
    "if not os.path.exists(pdf_path):\n",
    "    print(f\"[INFO] File doesn't exist, downloading...\")\n",
    "\n",
    "    # Enter the URL of the PDF\n",
    "    url = \"https://pressbooks.oer.hawaii.edu/humannutrition2/open/download?type=pdf\"\n",
    "\n",
    "    # THe Local filename to save the downloaded file\n",
    "    filename = pdf_path\n",
    "\n",
    "    # Send a GET request to the URL\n",
    "    responce = requests.get(url)\n",
    "\n",
    "    # Check if the request was successfull\n",
    "    if responce.status_code == 200:\n",
    "        # Open the file and save it\n",
    "        with open(filename , 'wb') as file:\n",
    "            file.write(responce.content)\n",
    "        print(f\"[INFO] The file has been download and saved as {filename}\")\n",
    "    else:\n",
    "        print(f\"[INFO] Falied to download the file . Status code {responce.status_code}\")\n",
    "\n",
    "else:\n",
    "    print(f\"File {pdf_path} exists.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def text_formaatter(text: str) -> str:\n",
    "    \"\"\" Performs minor formatting on text \"\"\"\n",
    "    cleaned_text = text.replace(\"\\n\" , \" \").strip()\n",
    "\n",
    "    # Pottentially more text formating functions can go here\n",
    "    return cleaned_text\n",
    "\n",
    "def open_and_read_pdf(pdf_path:str) -> list[dict]:\n",
    "    doc = fitz.open(pdf_path)\n",
    "    pages_and_text = []\n",
    "    for page_number , page in tqdm(enumerate(doc)):\n",
    "        text = page.get_text()\n",
    "        text = text_formaatter(text=text)\n",
    "        pages_and_text.append({\"page_number\" : page_number - 41 , \n",
    "                               \"page_char_count\": len(text) , \n",
    "                               \"page_word_count\": len(text.split(\" \")) , \n",
    "                               \"page_sentence_count_raw\": len(text.split(\". \")) , \n",
    "                               \"page_token_count\": len(text) / 4  , # 1 token = 4 character.\n",
    "                               \"text\": text})\n",
    "    \n",
    "    return pages_and_text\n",
    "\n",
    "pages_and_text = open_and_read_pdf(pdf_path=pdf_path)\n",
    "pages_and_text[:2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.sample(pages_and_text , k = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "df = pd.DataFrame(pages_and_text)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Further text processing (splitting pages into sentences)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "# Add a sentencizer pipeline\n",
    "nlp.add_pipe(\"sentencizer\")\n",
    "\n",
    "# Create a document instance as an example\n",
    "doc = nlp(\"This is a sentence. This is another sentenece. I like elephants.\")\n",
    "assert len(list(doc.sents)) == 3\n",
    "\n",
    "# Print out our sentence split\n",
    "list(doc.sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in tqdm(pages_and_text):\n",
    "    item['sentences'] = list(nlp(item[\"text\"]).sents)\n",
    "\n",
    "    # Make sure all sentences are string (the default type is a spacyy datatype)\n",
    "\n",
    "    item[\"sentences\"] = [str(sentence) for sentence in item[\"sentences\"]]\n",
    "\n",
    "    # Count the senteneces\n",
    "    item[\"page_sentence_count_spacy\"] = len(item[\"sentences\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.sample(pages_and_text , k = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(pages_and_text)\n",
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chucking our sentence together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define split size to turn groups of sentences into chunks\n",
    "num_sentence_chunk_size = 10\n",
    "\n",
    "# Create a function to split a list of texts recursively into chunk size\n",
    "\n",
    "def split_list(input_list: list[str] , slice_size : int = num_sentence_chunk_size) -> list[list[str]]:\n",
    "    return [input_list[i : i + slice_size] for i in range(0 , len(input_list) , slice_size)]\n",
    "\n",
    "test_list = list(range(25))\n",
    "split_list(test_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through pages and texts and split sentences into chunks\n",
    "for item in tqdm(pages_and_text):\n",
    "    item[\"sentence_chunks\"] = split_list(input_list=item[\"sentences\"] , slice_size=num_sentence_chunk_size)\n",
    "\n",
    "    item[\"num_chunks\"] = len(item[\"sentence_chunks\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.sample(pages_and_text , k = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(pages_and_text)\n",
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting each chunk into its own item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Split each chunk into own item\n",
    "pages_and_chunks = []\n",
    "for item in tqdm(pages_and_text):\n",
    "    for sentence_chunk in item[\"sentence_chunks\"]:\n",
    "        chunk_dict = {}\n",
    "        chunk_dict[\"page_number\"] = item[\"page_number\"]\n",
    "\n",
    "        # Join the sentences together into a paragraph-like structure, aka join the list of sentences into one paragraph\n",
    "        joined_sentence_chunk = \"\".join(sentence_chunk).replace(\"  \" , \" \").strip()\n",
    "        joined_sentence_chunk = re.sub(r'\\.([A-Z])' , r'. \\1' , joined_sentence_chunk) # \".A\" => \". A\" (will work for any capital letter)\n",
    "\n",
    "        chunk_dict[\"sentence_chunk\"] = joined_sentence_chunk\n",
    "\n",
    "        # Get some stats on our chunks\n",
    "        chunk_dict[\"chunk_char_count\"] = len(joined_sentence_chunk)\n",
    "        chunk_dict[\"chunk_word_count\"] = len([word for word in joined_sentence_chunk.split(\" \")])\n",
    "        chunk_dict[\"chunk_token_count\"] = len(joined_sentence_chunk) / 4 # 1 token = ~4 chars\n",
    "\n",
    "        pages_and_chunks.append(chunk_dict)\n",
    "\n",
    "len(pages_and_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.sample(pages_and_chunks , k = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(pages_and_chunks)\n",
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter chunks of text for short chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_token_length = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter our DataFrame for rows with under 30 tokens\n",
    "pages_and_chunks_over_min_token_len = df[df[\"chunk_token_count\"] > min_token_length].to_dict(orient = \"records\")\n",
    "pages_and_chunks_over_min_token_len[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.sample(pages_and_chunks_over_min_token_len , k = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding our text chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "embedding_model = SentenceTransformer(model_name_or_path = \"all-mpnet-base-v2\" , device = \"mps\")\n",
    "\n",
    "# Create a list of sentences\n",
    "sentences = [\"The Sentence Transformer library provides an easy way to create embeddings.\",\n",
    "\"Sentences can be embedded one by one or in a list.\",\n",
    "\"I like horses!\"]\n",
    "\n",
    "# Sentences are encoded/embedded by calling model.encode()\n",
    "embeddings = embedding_model.encode(sentences)\n",
    "embeddings_dict = dict(zip(sentences , embeddings))\n",
    "\n",
    "# See the embeddings\n",
    "for sentence , embedding in embeddings_dict.items():\n",
    "    print(f\"Sentences: {sentences}\")\n",
    "    print(f\"Embedding: {embedding}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "embedding_model.to(\"mps\")\n",
    "\n",
    "# Embed each chunk one by one\n",
    "for item in tqdm(pages_and_chunks_over_min_token_len):\n",
    "    item[\"embedding\"] = embedding_model.encode(item[\"sentence_chunk\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "text_chunks = [item['sentence_chunk'] for item in pages_and_chunks_over_min_token_len]\n",
    "text_chunks[523]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(text_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Embed all texts in batches\n",
    "text_chunks_embeddings = embedding_model.encode(text_chunks , batch_size=32 ,convert_to_tensor=True)\n",
    "\n",
    "text_chunks_embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save embeddings to file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_and_chunks_over_min_token_len[412]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save embeddings to file \n",
    "text_chunks_and_embeddings_df = pd.DataFrame(pages_and_chunks_over_min_token_len)\n",
    "embeddings_df_save_path = \"text_chunks_and_embeddings_df.csv\"\n",
    "text_chunks_and_embeddings_df.to_csv(embeddings_df_save_path , index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import saved file and view\n",
    "embeddings_df_save_path = \"/Users/pandhari/Desktop/NLP/RAG/text_chunks_and_embeddings_df.csv\"\n",
    "text_chunks_and_embeddings_df_load = pd.read_csv(embeddings_df_save_path)\n",
    "text_chunks_and_embeddings_df_load.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG - Search and Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "device = \"mps\"\n",
    "\n",
    "# Import texts and embedding df\n",
    "text_chunks_and_embeddings_df = pd.read_csv(\"/Users/pandhari/Desktop/NLP/RAG/text_chunks_and_embeddings_df.csv\")\n",
    "\n",
    "# Convert texts and embedding df to list of dicts\n",
    "pages_and_chunks = text_chunks_and_embeddings_df.to_dict(orient= \"records\")\n",
    "\n",
    "# Convert emedding column back to np.array (it got converted to string when it saved to CSV)\n",
    "text_chunks_and_embeddings_df[\"embedding\"] = text_chunks_and_embeddings_df[\"embedding\"].apply(lambda x: np.fromstring(x.strip(\"[]\") , sep=\" \"))\n",
    "\n",
    "# Convert texts and embedding df to list of dicts\n",
    "pages_and_chunks = text_chunks_and_embeddings_df.to_dict(orient=\"records\")\n",
    "\n",
    "# Convert embeddings to torch tensor and send to device (note: Numpy arrays are float64, torch tensors are float32 by default)\n",
    "embeddings = torch.tensor(np.array(text_chunks_and_embeddings_df[\"embedding\"].tolist()) , dtype=torch.float32).to(device=device)\n",
    "embeddings.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "from sentence_transformers import util , SentenceTransformer\n",
    "\n",
    "embedding_model = SentenceTransformer(model_name_or_path=\"all-mpnet-base-v2\" , device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define the query\n",
    "query = \"good foods for protein\"\n",
    "print(f\"Query: {query}\")\n",
    "\n",
    "# 2. Embed the query\n",
    "# Note: it's important to embed you query with the same model you embedding your passages\n",
    "query_embedding = embedding_model.encode(query , convert_to_tensor=True)\n",
    "query_embedding = query_embedding.to(device=device)\n",
    "\n",
    "# 3. Get Similaity socre with the dot product (use cosine similarity if outputs of model aren't normalize)\n",
    "from time import perf_counter as timer\n",
    "\n",
    "start_time = timer()\n",
    "dot_score = util.dot_score(a = query_embedding , b = embeddings)[0]\n",
    "end_time = timer()\n",
    "\n",
    "print(f\"[INFO] Time taken to get scores on {len(embeddings)} embeddings: {end_time - start_time:.5f} seconds\")\n",
    "\n",
    "# 4. Get the top-k results (we'll keep top 5)\n",
    "top_results = torch.topk(dot_score , k = 5)\n",
    "top_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_and_chunks[42]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "def print_wrapped(text , wrap_length = 80):\n",
    "    wrapped_text = textwrap.fill(text , wrap_length)\n",
    "    print(wrapped_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"good food for protein\"\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(\"Results:\")\n",
    "# Loop through zipped together scores and indices from torch.topk\n",
    "for score , idx in zip(top_results[0] , top_results[1]):\n",
    "    print(f\"Score: {score:.4f}\")\n",
    "    print(\"Text:\")\n",
    "    print_wrapped(pages_and_chunks[idx][\"sentence_chunk\"])\n",
    "    print(f\"Page Number: {pages_and_chunks[idx]['page_number']}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz # pymupdf Library\n",
    "\n",
    "# open PDF and Load target\n",
    "pdf_path = pdf_path\n",
    "doc = fitz.open(pdf_path)\n",
    "page = doc.load_page(411 + 41) # note: page numbers of our PDF start 41+\n",
    "\n",
    "# Get the image of the page\n",
    "img = page.get_pixmap(dpi = 300)\n",
    "\n",
    "# Save image (optional)\n",
    "# img.save(\"output_filename.png\")\n",
    "doc.close()\n",
    "\n",
    "# Convert the pixmap to a numpy array\n",
    "img_array = np.frombuffer(img.samples_mv , dtype=np.uint8).reshape(img.h , img.w , img.n)\n",
    "\n",
    "\n",
    "# Display the image using MatPlotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(13 , 10))\n",
    "plt.imshow(img_array)\n",
    "plt.title(f\"Query: {query} | Most relevant page:\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity measures: Dot Product and Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def dot_products(vector1 , vector2):\n",
    "    return torch.dot(vector1 , vector2)\n",
    "\n",
    "def cosine_similarity(vector1 , vector2):\n",
    "    dot_product = torch.dot(vector1 , vector2)\n",
    "\n",
    "    # Get Eucludian/L2 norm\n",
    "    norm_vector1 = torch.sqrt(torch.sum(vector1**2))\n",
    "    norm_vector2 = torch.sqrt(torch.sum(vector2**2))\n",
    "\n",
    "\n",
    "    return dot_product / (norm_vector1 * norm_vector2)\n",
    "\n",
    "# Examples vectors/tensors\n",
    "vector1 = torch.tensor([1 , 2 , 3] , dtype=torch.float32)\n",
    "vector2 = torch.tensor([1 , 2 , 3] , dtype=torch.float32)\n",
    "vector3 = torch.tensor([4 ,5 , 6] , dtype=torch.float32)\n",
    "vector4 = torch.tensor([-1 , -2 , -3] , dtype=torch.float32)\n",
    "\n",
    "# Calculate dot product\n",
    "print(\"Dot Product between vector1 and vector2 \" , dot_products(vector1 , vector2))\n",
    "print(\"Dot Product between vector1 and vector3 \" , dot_products(vector1 , vector3))\n",
    "print(\"Dot Product between vector1 and vector4 \" , dot_products(vector1 , vector4))\n",
    "\n",
    "# Calculate cosine similarity\n",
    "print(\"Cosine Similarity between vector1 and vector2 \" , cosine_similarity(vector1 , vector2))\n",
    "print(\"Cosine Similarity  between vector1 and vector3 \" , cosine_similarity(vector1 , vector3))\n",
    "print(\"Cosine Similarity  between vector1 and vector4 \" , cosine_similarity(vector1 , vector4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functionizing our semantic search pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_relevant_resources(query: str , embeddings: torch.tensor , model: SentenceTransformer = embedding_model , n_resources_to_return : int = 5 , print_time: bool = True):\n",
    "    \"\"\"Embeds a query with model and returns top k scores and indices from embeddings. \"\"\"\n",
    "\n",
    "    # Embed the query\n",
    "    query_embedding = model.encode(query , convert_to_tensor=True)\n",
    "    query_embedding = query_embedding.to(\"mps\")\n",
    "\n",
    "    embeddings = embeddings.to(\"mps\")\n",
    "\n",
    "    # Get dot products scores on embeddings\n",
    "    start_time = timer()\n",
    "    dot_scores = util.dot_score(query_embedding , embeddings)[0]\n",
    "    end_time = timer()\n",
    "\n",
    "    if print_time:\n",
    "        print(f\"[INFO] Time taken to get scores on ({len(embeddings)}) embeddings: {end_time - start_time:.5f} seconds.\")\n",
    "    \n",
    "    scores , indices = torch.topk(input=dot_scores , k = n_resources_to_return)\n",
    "\n",
    "    return scores , indices\n",
    "\n",
    "def print_top_results_and_scores(query: str , embeddings: torch.tensor , pages_and_chunks: list[dict] = pages_and_chunks , n_resources_to_return: int = 5):\n",
    "    \"\"\"\n",
    "    Finds relevant passages given a query and prints them out along with their scores. \n",
    "    \"\"\"\n",
    "    scores , indices = retrieve_relevant_resources(query=query , embeddings=embeddings , n_resources_to_return=n_resources_to_return)\n",
    "\n",
    "    # Loop through zipped together scores and indices from torch.topk\n",
    "    for score , idx in zip(scores ,indices):\n",
    "        print(f\"Score: {score:.4f}\")\n",
    "        print(\"Text:\")\n",
    "        print_wrapped(pages_and_chunks[idx][\"sentence_chunk\"])\n",
    "        print(f\"Page Number: {pages_and_chunks[idx]['page_number']}\")\n",
    "        print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_top_results_and_scores(query=\"foods high in fiber\" , embeddings=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting an LLM for Local generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get GPU available memory\n",
    "import torch\n",
    "gpu_memory_bytes = torch.cuda.get_device_properties(0).total_memory\n",
    "gpu_memory_gb = round(gpu_memory_bytes / (2**30))\n",
    "print(f\"Available GPU memory: {gpu_memory_gb}GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: the following is Gemma focused, however, there are more and more LLMs of the 2B and 7B size appearing for local use.\n",
    "if gpu_memory_gb < 5.1:\n",
    "    print(f\"Your available GPU memory is {gpu_memory_gb}GB, you may not have enough memory to run a Gemma LLM locally without quantization.\")\n",
    "elif gpu_memory_gb < 8.1:\n",
    "    print(f\"GPU memory: {gpu_memory_gb} | Recommended model: Gemma 2B in 4-bit precision.\")\n",
    "    use_quantization_config = True \n",
    "    model_id = \"google/gemma-2b-it\"\n",
    "elif gpu_memory_gb < 19.0:\n",
    "    print(f\"GPU memory: {gpu_memory_gb} | Recommended model: Gemma 2B in float16 or Gemma 7B in 4-bit precision.\")\n",
    "    use_quantization_config = False \n",
    "    model_id = \"google/gemma-2b-it\"\n",
    "elif gpu_memory_gb > 19.0:\n",
    "    print(f\"GPU memory: {gpu_memory_gb} | Recommend model: Gemma 7B in 4-bit or float16 precision.\")\n",
    "    use_quantization_config = False \n",
    "    model_id = \"google/gemma-7b-it\"\n",
    "\n",
    "print(f\"use_quantization_config set to: {use_quantization_config}\")\n",
    "print(f\"model_id set to: {model_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading an LLM locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer , AutoModelForCausalLM\n",
    "from transformers.utils import is_flash_attn_2_available\n",
    "\n",
    "# 1. Create a quantization config\n",
    "# Note: requires !pip install bitsandbytes accelerate\n",
    "from transformers import BitsAndBytesConfig\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True , bnb_4bit_compute_dtype=torch.float16)\n",
    "\n",
    "\n",
    "# Bonus: flash attention 2 = faster attention mechanism\n",
    "# Flash Attention 2 requires a GPU with a compute capability score of 8.0+\n",
    "if (is_flash_attn_2_available()) and (torch.cuda.get_device_capability()[0] >= 8):\n",
    "    attn_implementation = \"flash_attention_2\"\n",
    "else:\n",
    "    attn_implementation = \"sdpa\" # Scaled dot product attention\n",
    "\n",
    "# 2. Pick a model we'd like to use\n",
    "model_id = \"google/gemma-7b-it\"\n",
    "model_id = model_id\n",
    "\n",
    "# 3. Instantiate tokenizer (tokenizer turns text into tokens)\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_id)\n",
    "\n",
    "# 4. Instantiate the model\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=model_id , torch_dtype = torch.float16 , \n",
    "                                                 quantization_config = quantization_config if use_quantization_config else None , \n",
    "                                                 low_cpu_mem_usage = False , \n",
    "                                                 attn_implementation = attn_implementation)\n",
    "\n",
    "\n",
    "\n",
    "if not use_quantization_config:\n",
    "    llm_model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install bitsandbytes accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_num_params(model: torch.nn.Module):\n",
    "    return sum([param.numel() for param in model.parameters()])\n",
    "\n",
    "get_model_num_params(llm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_mem_size(model: torch.nn.Module):\n",
    "    # Get model parameters and buffer sizes\n",
    "    mem_params = sum([param.nelement() * param.element_size() for param in model.parameters()])\n",
    "    mem_buffers = sum([buf.nelement() * buf.element_size() for buf in model.buffers()])\n",
    "\n",
    "    # Calculate model sizes\n",
    "    model_mem_bytes = mem_params + mem_buffers\n",
    "    model_mem_mb = model_mem_bytes / (1024**2)\n",
    "    model_mem_gb = model_mem_bytes / (1024**3)\n",
    "\n",
    "    return {\"model_mem_bytes\": model_mem_bytes , \n",
    "            \"model_mem_mb\": round(model_mem_mb , 2) , \n",
    "            \"model_mem_gb\": round(model_mem_gb , 2)}\n",
    "\n",
    "\n",
    "get_model_mem_size(llm_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating text with our LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"What are the macronutrients, and what roles do they play in the human body?\"\n",
    "print(f\"Input text : \\n{input_text}\")\n",
    "\n",
    "# Create prompt template for instruction-tuned model\n",
    "dialogue_template = [\n",
    "    {\"role\":\"user\" , \n",
    "     \"content\": input_text}\n",
    "]\n",
    "\n",
    "# Apply the chat template\n",
    "prompt = tokenizer.apply_chat_template(conversation=dialogue_template , \n",
    "                                       tokenize=False , \n",
    "                                       add_generation_prompt=True)\n",
    "\n",
    "print(f\"\\n Prompt (formated): \\n {prompt} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Tokenize the input text (turn it into numbers) and send it to the GPU\n",
    "input_ids = tokenizer(prompt , \n",
    "    return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "# Generate outputs from local LLM\n",
    "outputs = llm_model.generate(**input_ids , max_new_tokens = 256)\n",
    "\n",
    "print(f\"Model output (toknes): \\n {outputs[0]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode the output tokens to text\n",
    "outputs_decoded = tokenizer.decode(outputs[0])\n",
    "print(f\"Model output (decoded): \\n {outputs_decoded}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmenting our prompt with context items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_formatter(query:str , \n",
    "                     context_items: list[dict]) -> str:\n",
    "    context = \"_ \" + "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
